import pandas as pd
import hashlib
import re
import jieba
from typing import List, Optional, Tuple


def load_csv(file_path: str) -> pd.DataFrame:
    """
    è¯»å– CSV æ–‡ä»¶ï¼Œå¹¶è¿”å› DataFrame
    """
    try:
        df = pd.read_csv(file_path)
        print("åˆå§‹æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š")
        df.info()
        unique_question_count = df["é—®é¢˜æ ‡é¢˜"].nunique()
        print(f"é—®é¢˜æ ‡é¢˜çš„æ•°é‡ï¼š{unique_question_count}")
        return df
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚")
        return pd.DataFrame()
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return pd.DataFrame()


def read_text_file(file_path: str) -> Optional[List[str]]:
    """
    è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹ï¼Œæ¯ä¸€è¡Œæ„æˆä¸€ä¸ªåˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
        return [line.strip() for line in lines if line.strip()]
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚")
        return None
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def write_text_file(file_path: str, content: List[str]) -> None:
    """
    å°†å†…å®¹åˆ—è¡¨å†™å…¥æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯ä¸ªå…ƒç´ ä¸€è¡Œ
    """
    try:
        with open(file_path, 'w', encoding='utf-8') as file:
            for line in content:
                file.write(line + "\n")
        print(f"æ•°æ®å·²æˆåŠŸä¿å­˜ä¸º '{file_path}'")
    except Exception as e:
        print(f"å†™å…¥æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def calculate_hash(text: str) -> str:
    """
    è®¡ç®—æ–‡æœ¬çš„ SHA-256 å“ˆå¸Œå€¼
    """
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def clean_text(text):
    """
    æ¸…æ´—æ–‡æœ¬å†…å®¹
    """
    if pd.isnull(text):
        return ""

    text = re.sub(r'<.*?>', '', text)  # HTMLæ ‡ç­¾
    text = re.sub(r'&[a-z]+;', '', text)  # HTMLè½¬ä¹‰å­—ç¬¦
    text = re.sub(r'http\S+|www\S+', '', text)  # URLé“¾æ¥
    text = re.sub(r'pic\.\S+|img\.\S+', '', text)  # å›¾ç‰‡é“¾æ¥
    text = re.sub(r'â†“', '', text)
    text = re.sub(r'\d+', '', text)  # ç§»é™¤æ•°å­—
    text = re.sub(r'\s+', ' ', text)  # å¤šä½™ç©ºç™½ç¬¦
    return text.strip()


def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ•°æ®é¢„å¤„ç†
    """
    # å¤„ç†èµåŒæ•°å’Œè¯„è®ºæ•°
    df["èµåŒæ•°"] = df["èµåŒæ•°"].astype(str).str.replace(
        r"\s|â€‹|,|ä¸ª|ğŸ‘|èµ|\+", "", regex=True
    )
    df["è¯„è®ºæ•°"] = df["è¯„è®ºæ•°"].astype(str).str.replace(
        r"\s|â€‹|,|æ¡|è¯„è®º", "", regex=True
    )

    # è½¬æ¢ä¸ºæ•°å­—ç±»å‹ï¼Œæ— æ³•è½¬æ¢çš„è®¾ä¸º NaN
    df["èµåŒæ•°"] = pd.to_numeric(df["èµåŒæ•°"], errors="coerce")
    df["è¯„è®ºæ•°"] = pd.to_numeric(df["è¯„è®ºæ•°"], errors="coerce")

    # æ ‡å‡†åŒ–â€œå›ç­”æ—¶é—´â€å­—æ®µ
    df["å›ç­”æ—¶é—´"] = df["å›ç­”æ—¶é—´"].astype(str).str.extract(r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2})")
    df["å›ç­”æ—¶é—´"] = pd.to_datetime(df["å›ç­”æ—¶é—´"], errors="coerce")

    # åˆ é™¤å®Œå…¨ç¼ºå¤±â€œå›ç­”å†…å®¹â€çš„è®°å½•
    df = df.dropna(subset=["å›ç­”å†…å®¹"])

    # å¡«å……å…¶ä»–ç¼ºå¤±å€¼
    df.loc[:, "é—®é¢˜å†…å®¹"] = df["é—®é¢˜å†…å®¹"].fillna("")
    df.loc[:, "ç­”ä¸»æ˜µç§°"] = df["ç­”ä¸»æ˜µç§°"].fillna("åŒ¿***")
    df.loc[:, "å›ç­”æ—¶é—´"] = df["å›ç­”æ—¶é—´"].fillna("2025-05-08 16:23:00")
    df.loc[:, "èµåŒæ•°"] = df["èµåŒæ•°"].fillna("0")
    df.loc[:, "è¯„è®ºæ•°"] = df["è¯„è®ºæ•°"].fillna("0")

    return df


def filter_and_group(
    df: pd.DataFrame
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    æ•°æ®è¿‡æ»¤ä¸åˆ†ç»„
    """
    # åº”ç”¨æ–‡æœ¬æ¸…æ´—åˆ°â€œå›ç­”å†…å®¹â€
    df["å›ç­”å†…å®¹"] = df["å›ç­”å†…å®¹"].apply(clean_text)

    # æ·»åŠ å“ˆå¸Œåˆ—ï¼Œç”¨äºåç»­æ•°æ®å»é‡
    df["content_hash"] = df["å›ç­”å†…å®¹"].apply(calculate_hash)

    # æ£€æŸ¥å¹¶è½¬æ¢èµåŒæ•°ä¸ºæ•°å€¼ç±»å‹
    df["èµåŒæ•°"] = pd.to_numeric(df["èµåŒæ•°"], errors="coerce").fillna(0)

    # æŒ‰èµåŒæ•°æ’åºå¹¶å»é‡
    df = (
        df.sort_values("èµåŒæ•°", ascending=False)
        .drop_duplicates(subset="content_hash")
        .drop(columns=["content_hash"])
        .reset_index(drop=True)
    )

    print("æ•°æ®å¤„ç†å®Œæˆï¼")

    # å‰”é™¤å›ç­”å†…å®¹å°‘äº15å­—çš„è®°å½•
    df = df[df["å›ç­”å†…å®¹"].str.len() >= 15]
    df = df.reset_index(drop=True)
    print(f"å‰”é™¤è¿‡çŸ­è¯„è®ºåå‰©ä½™ {len(df)} æ¡è®°å½•")

    # æŒ‰ç…§â€œé—®é¢˜æ ‡é¢˜â€å¯¹å›ç­”å†…å®¹åˆå¹¶
    df_grouped = (
        df.groupby("é—®é¢˜æ ‡é¢˜")["å›ç­”å†…å®¹"].apply(lambda x: " ".join(x)).reset_index()
    )

    # å¯¹æ¯ä¸ªé—®é¢˜ç»Ÿè®¡å›ç­”æ•°
    question_counts = df["é—®é¢˜æ ‡é¢˜"].value_counts().reset_index()
    question_counts.columns = ["é—®é¢˜æ ‡é¢˜", "å›ç­”æ•°"]
    question_counts_sorted = question_counts.sort_values(by="å›ç­”æ•°", ascending=False)

    return df, df_grouped, question_counts_sorted


def segment_text_with_dict(raw_words_file: str, dict_file: str, output_file: str) -> None:
    """
    ä½¿ç”¨è‡ªå®šä¹‰è¯å…¸å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    """
    # åŠ è½½è‡ªå®šä¹‰è¯å…¸
    jieba.load_userdict(dict_file)

    # è¯»å–åŸå§‹æ–‡æœ¬
    raw_words = read_text_file(raw_words_file)
    if not raw_words:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ '{raw_words_file}' æˆ–æ–‡ä»¶å†…å®¹ä¸ºç©ºã€‚")
        return

    # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
    segmented_words = [" ".join(jieba.cut(line)) for line in raw_words]

    # ä¿å­˜åˆ†è¯ç»“æœ
    write_text_file(output_file, segmented_words)


def remove_stopwords(segmented_file: str, stopwords_file: str, output_file: str) -> None:
    """
    ä½¿ç”¨åœç”¨è¯è¡¨å¯¹åˆ†è¯åçš„æ–‡æœ¬è¿›è¡Œå»é™¤åœç”¨è¯å¤„ç†
    """
    # è¯»å–åœç”¨è¯è¡¨
    stopwords = set(read_text_file(stopwords_file) or [])
    if not stopwords:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½åœç”¨è¯è¡¨ '{stopwords_file}' æˆ–æ–‡ä»¶å†…å®¹ä¸ºç©ºã€‚")
        return

    # è¯»å–åˆ†è¯ç»“æœ
    segmented_words = read_text_file(segmented_file)
    if not segmented_words:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ '{segmented_file}' æˆ–æ–‡ä»¶å†…å®¹ä¸ºç©ºã€‚")
        return

    # å»é™¤åœç”¨è¯
    filtered_words = [
        " ".join(word for word in line.split() if word not in stopwords)
        for line in segmented_words
    ]

    # ä¿å­˜å¤„ç†åçš„ç»“æœ
    write_text_file(output_file, filtered_words)


if __name__ == "__main__":
    input_file = "feet_file\\raw_data.csv"
    cleaned_words_file = "feet_file\\raw_words.txt"
    dict_file = "tool_file\\diydict.txt"
    segmented_words_file = "output_file\\segmented_words.txt"
    stopwords_file = "tool_file\\stopwords.txt"
    filtered_words_file = "output_file\\filtered_words.txt"

    # Load the data
    df = load_csv(input_file)

    # Preprocess the data
    df = preprocess_data(df)

    # Perform filtering and grouping
    cleaned_df, grouped_df, question_counts = filter_and_group(df)

    # Save the cleaned "å›ç­”å†…å®¹" to a text file
    write_text_file(cleaned_words_file, cleaned_df["å›ç­”å†…å®¹"].tolist())

    # Perform segmentation using the custom dictionary
    segment_text_with_dict(cleaned_words_file, dict_file, segmented_words_file)

    # Remove stopwords from the segmented text
    remove_stopwords(segmented_words_file, stopwords_file, filtered_words_file)

    # Print the top 10 question titles and their respective answer counts
    print("å‰10ä¸ªé—®é¢˜æ ‡é¢˜åŠå…¶å›ç­”æ•°:")
    print(question_counts.head(10))

